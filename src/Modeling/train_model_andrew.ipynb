{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data labels are in the form below:\n",
    "alphabet = ['1', '2', '3','4', 'space', 'del', \n",
    "'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "labels_ = alphabet\n",
    "NUM_CLASSES = len(labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_flat = pd.read_csv(\"./data/train/x_data900.csv\") \n",
    "y_flat = pd.read_csv(\"./data/train/y_data900.csv\")  #Note the first column of this is the index, so it's nx2 size\n",
    "\n",
    "x_flat_test = pd.read_csv(\"./data/test/x_data_test100.csv\")\n",
    "y_flat_test = pd.read_csv(\"./data/test/y_data_test100.csv\") #Note the first column of this is the index, so it's nx2 size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]],\n",
       "\n",
       "       [[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]],\n",
       "\n",
       "       [[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to unflatten\n",
    "def unflatten(df,shape):\n",
    "    \"\"\" \n",
    "    reshape a dataframe to the correct array size\n",
    "    shape is an array ex [21,3] which describes the desired shape\n",
    "    \"\"\"\n",
    "    old_array = df.to_numpy()\n",
    "    new_array = []\n",
    "    for point in old_array:\n",
    "        point_reshaped = point.reshape(shape[0], shape[1])\n",
    "        new_array.append(point_reshaped)\n",
    "\n",
    "    return np.array(new_array)\n",
    "\n",
    "# TEST\n",
    "arr = np.array([[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8],[1,2,3,4,5,6,7,8]])\n",
    "arr = pd.DataFrame(arr)\n",
    "unflatten(arr,shape=[2,4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (24367, 21, 3) training score's shape (24367, 1)\n",
      "testing data shape: (2659, 21, 3) testing score's shape (2659, 1)\n"
     ]
    }
   ],
   "source": [
    "#revert the data back into its original form\n",
    "x_data = unflatten(x_flat,shape=[21,3])\n",
    "X_test = unflatten(x_flat_test,shape=[21,3])\n",
    "\n",
    "#Adjust labels to numpy array\n",
    "y_data = y_flat.to_numpy()\n",
    "y_test = y_flat_test.to_numpy()\n",
    "\n",
    "print(\"training data shape:\", np.shape(x_data), \"training score's shape\", np.shape(y_data))\n",
    "print(\"testing data shape:\", np.shape(X_test), \"testing score's shape\", np.shape(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x_data, y_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode\n",
    "y_train = to_categorical(y_train).astype(int)\n",
    "y_valid = to_categorical(y_valid).astype(int)\n",
    "y_test = to_categorical(y_test).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our models. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Our inputs are:\n",
    "- X_Train, y_train \n",
    "- X_valid, y_valid\n",
    "- X_test,y_test\n",
    "Each datapoint in the Xs are [21,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "checkpoint_path = \"./checkpoints.ckpt/\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "#Compile\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "667/686 [============================>.] - ETA: 0s - loss: 1.0454 - categorical_accuracy: 0.7656\n",
      "Epoch 1: saving model to .\\checkpoints.ckpt\n",
      "686/686 [==============================] - 3s 4ms/step - loss: 1.0255 - categorical_accuracy: 0.7708 - val_loss: 0.2847 - val_categorical_accuracy: 0.9479\n",
      "Epoch 2/5\n",
      "672/686 [============================>.] - ETA: 0s - loss: 0.2088 - categorical_accuracy: 0.9695\n",
      "Epoch 2: saving model to .\\checkpoints.ckpt\n",
      "686/686 [==============================] - 2s 3ms/step - loss: 0.2068 - categorical_accuracy: 0.9698 - val_loss: 0.1422 - val_categorical_accuracy: 0.9795\n",
      "Epoch 3/5\n",
      "683/686 [============================>.] - ETA: 0s - loss: 0.1367 - categorical_accuracy: 0.9801\n",
      "Epoch 3: saving model to .\\checkpoints.ckpt\n",
      "686/686 [==============================] - 2s 3ms/step - loss: 0.1365 - categorical_accuracy: 0.9802 - val_loss: 0.1044 - val_categorical_accuracy: 0.9819\n",
      "Epoch 4/5\n",
      "666/686 [============================>.] - ETA: 0s - loss: 0.1028 - categorical_accuracy: 0.9852\n",
      "Epoch 4: saving model to .\\checkpoints.ckpt\n",
      "686/686 [==============================] - 2s 3ms/step - loss: 0.1031 - categorical_accuracy: 0.9851 - val_loss: 0.0686 - val_categorical_accuracy: 0.9877\n",
      "Epoch 5/5\n",
      "679/686 [============================>.] - ETA: 0s - loss: 0.0857 - categorical_accuracy: 0.9866\n",
      "Epoch 5: saving model to .\\checkpoints.ckpt\n",
      "686/686 [==============================] - 2s 3ms/step - loss: 0.0853 - categorical_accuracy: 0.9866 - val_loss: 0.0711 - val_categorical_accuracy: 0.9877\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x138cb96e460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, callbacks=[cp_callback],validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9845806956291199\n",
      "Loss: 0.08085747808218002\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set:\n",
    "score = model.evaluate(np.array(X_test), y_test, verbose = 0) \n",
    "print(f\"Accuracy: {score[1]}\")\n",
    "print(f\"Loss: {score[0]}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Demo\n",
    "Now let's see how this performs live!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_hands.Hands(\n",
    "        model_complexity=0,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        landmarks = []\n",
    "        letter = ''\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style())\n",
    "                \n",
    "                # Get the bounding box of the hand\n",
    "                x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y = landmark.x, landmark.y\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "\n",
    "                # Normalize the coordinates with respect to the bounding box of the hand\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x, y, z = landmark.x, landmark.y, landmark.z\n",
    "                    x_norm = (x - x_min) / (x_max - x_min)\n",
    "                    y_norm = (y - y_min) / (y_max - y_min)\n",
    "                    landmarks.append([x_norm, y_norm, z])\n",
    "\n",
    "            # Convert the landmarks to a feature vector\n",
    "            x_t = np.array(landmarks)\n",
    "                        \n",
    "            \n",
    "            if x_t.flatten().shape[0] == 63:\n",
    "            \n",
    "                res_ = model.predict(np.array([x_t]), verbose = 0)\n",
    "\n",
    "                index_class = np.argmax(res_)\n",
    "\n",
    "                letter = alphabet[index_class]\n",
    "\n",
    "\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        \n",
    "        \n",
    "        font                   = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        bottomLeftCornerOfText = (100,50)\n",
    "        fontScale              = 1\n",
    "        fontColor              = (0,0,0)\n",
    "        thickness              = 4\n",
    "        lineType               = 2\n",
    "\n",
    "        image = cv2.putText(cv2.flip(image, 1),letter, \n",
    "            bottomLeftCornerOfText, \n",
    "            font, \n",
    "            fontScale,\n",
    "            fontColor,\n",
    "            thickness,\n",
    "            lineType)\n",
    "        \n",
    "        \n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "            \n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

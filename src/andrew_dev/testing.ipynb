{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hel stairs intermittent Habit hauled hauledRocketdit Habit vendors\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "\n",
    "# define prefix\n",
    "prefix = \"hel\"\n",
    "\n",
    "# tokenize prefix and convert to tensor\n",
    "input_ids = torch.tensor(tokenizer.encode(prefix)).unsqueeze(0)\n",
    "\n",
    "# generate completions\n",
    "max_length = len(prefix) + 5 # set maximum length of completion\n",
    "temperature = 1.0 # set sampling temperature\n",
    "top_k = 10 # set top-k sampling value\n",
    "top_p = 0.95 # set top-p sampling value\n",
    "eos_token_id = tokenizer.eos_token_id # get end-of-sequence token id\n",
    "\n",
    "# generate logits for next token using model\n",
    "logits = model(input_ids)[0][:, -1, :]\n",
    "# apply top-k and top-p filtering to logits\n",
    "filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "# sample next token from filtered logits\n",
    "probabilities = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "\n",
    "# append next token to output tensor\n",
    "output = torch.cat((input_ids, next_token), dim=1)\n",
    "\n",
    "# generate sequence of tokens\n",
    "for i in range(max_length):\n",
    "    # generate logits for next token using model\n",
    "    logits = model(output)[0][:, -1, :]\n",
    "    # apply top-k and top-p filtering to logits\n",
    "    filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "    # sample next token from filtered logits\n",
    "    probabilities = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "    next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "    # append next token to output tensor\n",
    "    output = torch.cat((output, next_token), dim=1)\n",
    "    # check if end-of-sequence token is generated\n",
    "    if next_token == eos_token_id:\n",
    "        break\n",
    "\n",
    "# decode and print completion\n",
    "completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hel Prob Participation directly credibility circumcisedmediately conservation ProbJD heir TAmediately intermittent\n",
      "hel Observoho ONE Participation Hancock autonomy hauled ONE Observ Prob trilogy circumcised Rh\n",
      "hel reviewingScene Rhdit intermittent credibilityRocket Brew Jr stairs circumcised ESV heir\n",
      "hel confiriken intermittent credibility trilogy trilogy scalppress Hancock TAreement subst ONE\n",
      "helmediately TAhibit ONE hauled stairsoother circumciseddit Brew Habit intermittentdit\n",
      "hel antibioticmediately stairs conservationSher004 Prob antibiotic antibioticting circumcised Participation Motorola\n",
      "hel vendors trilogy Brew directlyRocket heir Observ conservation intermittentatisf hauledootherpress\n",
      "hel TA TA pawn Observpress ObservJDootherpress autonomyoho Moneyimura\n",
      "hel Jratisf antibioticreement vendors reviewing antibioticRocket directly Observoother scalp Observ\n",
      "heltingSheratisf hauled confirJDRocketmediately reviewing Rh confir Hancock ONE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "# define prefix\n",
    "prefix = \"hel\"\n",
    "\n",
    "# tokenize prefix and convert to tensor\n",
    "input_ids = torch.tensor(tokenizer.encode(prefix)).unsqueeze(0)\n",
    "\n",
    "# generate completions\n",
    "max_length = len(prefix) + 10 # set maximum length of completion\n",
    "temperature = 1.0 # set sampling temperature\n",
    "top_k = 50 # set top-k sampling value\n",
    "top_p = 0.95 # set top-p sampling value\n",
    "eos_token_id = tokenizer.eos_token_id # get end-of-sequence token id\n",
    "\n",
    "for i in range(10): # generate 10 completions\n",
    "    output = input_ids # initialize output tensor\n",
    "    for _ in range(max_length):\n",
    "        # generate logits for next token using model\n",
    "        logits = model(output)[0][:, -1, :]\n",
    "        # apply top-k and top-p filtering to logits\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "        # sample next token from filtered logits\n",
    "        probabilities = torch.softmax(filtered_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        # append next token to output tensor\n",
    "        output = torch.cat((output, next_token), dim=1)\n",
    "        # check if end-of-sequence token is generated\n",
    "        if next_token == eos_token_id:\n",
    "            break\n",
    "    # decode and print completion\n",
    "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['helimura subst Prob Daniel reviewing dispatchpressdit stairs directlySherdit', 'hel Observ ESV pawn substimura dispatch Observ antibiotic directly ESV Motorola hauled', 'hel MotorolaRocketoho Jroho Rh Brew hauled Observ ONE confiroho', 'helhibit TA hauled confir ProbJDikenootherpress Money scalp heir', 'helhibitatisf Brew Brew hauledSceneScene autonomy HabitRocketimuraSher', 'hel ESV Habit TA Brew Prob004 autonomy Jr vendorsatisf Hancock ESV', 'hel Habit Motorola reviewingoother subst Jr Daniel subst stairs antibioticikenScene', 'hel Daniel directlyRocket intermittent autonomy dispatch dispatch004press confir Observ conservation', 'hel004 heirhibit confir autonomypressiken MotorolaJD Danielhibit confir', 'hel intermittentpress confir Hancockditreement ESV conservation Jr stairs vendors intermittent']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('sshleifer/tiny-gpt2')\n",
    "\n",
    "# define prefix\n",
    "prefix = \"hel\"\n",
    "\n",
    "# tokenize prefix and convert to tensor\n",
    "input_ids = tokenizer.encode(prefix, return_tensors='pt')\n",
    "\n",
    "# generate completions\n",
    "max_length = len(prefix) + 10 # set maximum length of completion\n",
    "temperature = 1.0 # set sampling temperature\n",
    "top_k = 50 # set top-k sampling value\n",
    "top_p = 0.95 # set top-p sampling value\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=max_length,\n",
    "    temperature=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=10,\n",
    ")\n",
    "\n",
    "# decode and print completions\n",
    "completions = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "print(completions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images to live image landmarks version:\n",
    "Below takes in an array where each element is the file name of an image in the folder data/unlabled/. The script reads the image, runs a hand detection algorithm, then overlays the results and saves the output to the folder data/labeled/ with the original file name + \"_marked.png\"\n",
    "\n",
    "\n",
    "TODO: Save the keypoints into a separate csv to create a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 file, ./data/unlabeled/hands3.jpg\n",
      "[landmark {\n",
      "  x: 0.5648515224456787\n",
      "  y: 0.7065058946609497\n",
      "  z: -1.6709475403331453e-06\n",
      "}\n",
      "landmark {\n",
      "  x: 0.43671467900276184\n",
      "  y: 0.6010831594467163\n",
      "  z: -0.05960231274366379\n",
      "}\n",
      "landmark {\n",
      "  x: 0.3417746424674988\n",
      "  y: 0.495012104511261\n",
      "  z: -0.07195031642913818\n",
      "}\n",
      "landmark {\n",
      "  x: 0.3244509696960449\n",
      "  y: 0.3697652220726013\n",
      "  z: -0.0785597711801529\n",
      "}\n",
      "landmark {\n",
      "  x: 0.40388110280036926\n",
      "  y: 0.31218308210372925\n",
      "  z: -0.062408093363046646\n",
      "}\n",
      "landmark {\n",
      "  x: 0.4755207300186157\n",
      "  y: 0.37364351749420166\n",
      "  z: 0.009500401094555855\n",
      "}\n",
      "landmark {\n",
      "  x: 0.4729347229003906\n",
      "  y: 0.29737287759780884\n",
      "  z: -0.050254132598638535\n",
      "}\n",
      "landmark {\n",
      "  x: 0.44780904054641724\n",
      "  y: 0.4066506624221802\n",
      "  z: -0.07407911121845245\n",
      "}\n",
      "landmark {\n",
      "  x: 0.4525124132633209\n",
      "  y: 0.40707287192344666\n",
      "  z: -0.08487633615732193\n",
      "}\n",
      "landmark {\n",
      "  x: 0.5736780166625977\n",
      "  y: 0.3985717296600342\n",
      "  z: 0.01609756425023079\n",
      "}\n",
      "landmark {\n",
      "  x: 0.565618097782135\n",
      "  y: 0.3255450129508972\n",
      "  z: -0.04090666398406029\n",
      "}\n",
      "landmark {\n",
      "  x: 0.5232690572738647\n",
      "  y: 0.4567101001739502\n",
      "  z: -0.03766270726919174\n",
      "}\n",
      "landmark {\n",
      "  x: 0.5311375856399536\n",
      "  y: 0.4425356388092041\n",
      "  z: -0.023370403796434402\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6537870168685913\n",
      "  y: 0.430669903755188\n",
      "  z: 0.004705762956291437\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6541537642478943\n",
      "  y: 0.36119335889816284\n",
      "  z: -0.055572427809238434\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6035833358764648\n",
      "  y: 0.47337424755096436\n",
      "  z: -0.024717207998037338\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6116862893104553\n",
      "  y: 0.47228822112083435\n",
      "  z: 0.011752210557460785\n",
      "}\n",
      "landmark {\n",
      "  x: 0.736675500869751\n",
      "  y: 0.475811243057251\n",
      "  z: -0.014165539294481277\n",
      "}\n",
      "landmark {\n",
      "  x: 0.7462303638458252\n",
      "  y: 0.39417487382888794\n",
      "  z: -0.033394306898117065\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6926819086074829\n",
      "  y: 0.4598965346813202\n",
      "  z: -0.0025248369202017784\n",
      "}\n",
      "landmark {\n",
      "  x: 0.6830735206604004\n",
      "  y: 0.4876154959201813\n",
      "  z: 0.027233436703681946\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# For static images:\n",
    "IMAGE_FILES = [\"hands3.jpg\"]\n",
    "with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5) as hands:\n",
    "  for idx, file_ori in enumerate(IMAGE_FILES):\n",
    "    file = \"./data/unlabeled/\" + file_ori\n",
    "    print(\"iteration\",idx,\"file,\",file)\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "\n",
    "    # image = cv2.flip(cv2.imread(file), 1)\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "      print(results.multi_hand_landmarks)\n",
    "      for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # landmarks.append()\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "    else:\n",
    "      print(\"no landmarks found for\",file)\n",
    "\n",
    "    cv2.imwrite(\"./data/labeled/\" +str(file_ori)[:-4]+ \"marked.png\",cv2.flip(image, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want the relative normalized coordinates of thes hand so it is invariant to location and image size!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 file, ./data/unlabeled/hands.jpg\n",
      "[ 1.00000000e+00  4.34967835e-01 -3.47916576e-08  8.82582635e-01\n",
      "  7.59323158e-01 -1.83860995e-02  6.89516073e-01  9.33540385e-01\n",
      " -2.44351812e-02  5.23791267e-01  9.92139709e-01 -3.15571316e-02\n",
      "  3.82077427e-01  1.00000000e+00 -3.75392064e-02  5.04185301e-01\n",
      "  6.06368761e-01  6.72204792e-03  3.23579050e-01  7.12554842e-01\n",
      " -1.75520517e-02  2.25997013e-01  8.39138381e-01 -4.18161079e-02\n",
      "  1.51806618e-01  9.76945895e-01 -5.76517135e-02  4.77653627e-01\n",
      "  4.08366269e-01  7.47234328e-04  2.59541127e-01  4.45197047e-01\n",
      " -1.72595009e-02  1.14985974e-01  5.40388545e-01 -3.80484127e-02\n",
      "  0.00000000e+00  6.53343661e-01 -5.24306186e-02  4.98473873e-01\n",
      "  2.32378382e-01 -1.09612327e-02  2.92032793e-01  2.19396905e-01\n",
      " -2.99397297e-02  1.55154329e-01  2.84264374e-01 -4.68715392e-02\n",
      "  4.01836178e-02  3.75865428e-01 -5.75676374e-02  5.54889918e-01\n",
      "  6.97741740e-02 -2.51207817e-02  3.97184439e-01  0.00000000e+00\n",
      " -4.07977998e-02  2.82935436e-01  1.45630962e-02 -4.96993512e-02\n",
      "  1.79324748e-01  7.03180477e-02 -5.52067496e-02  0.00000000e+00\n",
      "  4.80940937e-01  3.98184341e-07  1.97625860e-01  8.26904867e-01\n",
      " -2.14029737e-02  4.38820730e-01  1.00000000e+00 -2.06197482e-02\n",
      "  6.70471861e-01  9.52557192e-01 -1.98393129e-02  7.71836382e-01\n",
      "  7.50240137e-01 -1.95069965e-02  5.90074547e-01  7.37779251e-01\n",
      "  2.76217237e-02  7.77202046e-01  7.30893867e-01  2.22558714e-02\n",
      "  8.84344541e-01  6.81433434e-01  6.18365919e-03  9.75890900e-01\n",
      "  6.30351591e-01 -8.00790824e-03  5.75775047e-01  5.18268900e-01\n",
      "  2.87939105e-02  7.62673635e-01  4.67886232e-01  2.39749812e-02\n",
      "  8.98188697e-01  4.25146092e-01  1.00762828e-03  1.00000000e+00\n",
      "  3.89454661e-01 -1.89578235e-02  5.18478589e-01  3.09097966e-01\n",
      "  2.42492016e-02  6.90073939e-01  2.38064693e-01  1.41587360e-02\n",
      "  8.05605254e-01  2.05899381e-01 -6.12546317e-03  8.85914699e-01\n",
      "  1.95354546e-01 -2.19626967e-02  4.26012306e-01  9.15891101e-02\n",
      "  1.70404203e-02  5.67018302e-01  9.23596137e-03  5.26789110e-03\n",
      "  6.61384141e-01  0.00000000e+00 -6.25683973e-03  7.42912716e-01\n",
      "  2.88980437e-02 -1.58031173e-02]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For static images:\n",
    "IMAGE_FILES = [\"hands.jpg\"]\n",
    "with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=2,\n",
    "        min_detection_confidence=0.5) as hands:\n",
    "  for idx, file_ori in enumerate(IMAGE_FILES):\n",
    "    file = \"./data/unlabeled/\" + file_ori\n",
    "    print(\"iteration\",idx,\"file,\",file)\n",
    "    # Read an image, flip it around y-axis for correct handedness output (see\n",
    "    # above).\n",
    "\n",
    "    # image = cv2.flip(cv2.imread(file), 1)\n",
    "    image = cv2.imread(file)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert the BGR image to RGB before processing.\n",
    "    results = hands.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # Extract the hand landmarks\n",
    "    landmarks = []\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get the bounding box of the hand\n",
    "            x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y = landmark.x, landmark.y\n",
    "                x_min = min(x_min, x)\n",
    "                y_min = min(y_min, y)\n",
    "                x_max = max(x_max, x)\n",
    "                y_max = max(y_max, y)\n",
    "            \n",
    "            # Normalize the coordinates with respect to the bounding box of the hand\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                x, y, z = landmark.x, landmark.y, landmark.z\n",
    "                x_norm = (x - x_min) / (x_max - x_min)\n",
    "                y_norm = (y - y_min) / (y_max - y_min)\n",
    "                landmarks.append([x_norm, y_norm, z])\n",
    "\n",
    "    # Convert the landmarks to a feature vector\n",
    "    feature_vector = np.array(landmarks).flatten()\n",
    "\n",
    "    # Print the feature vector\n",
    "    print(feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should use the above and create a csv training set where, for valid cases, we save the label (ex \"A\") as well as the extracted features, from the image using the above code, as a row in the file. When we want to use this file for training, we simply need to set the first column to the labels and the remaining columns to be the features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U7_jpg.rf.0037faea78f8a89329a93006132921b3.jpg</td>\n",
       "      <td>390</td>\n",
       "      <td>390</td>\n",
       "      <td>U</td>\n",
       "      <td>38</td>\n",
       "      <td>69</td>\n",
       "      <td>347</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P12_jpg.rf.0046c1c30abbbccd31716c5b2ad835b9.jpg</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>P</td>\n",
       "      <td>84</td>\n",
       "      <td>203</td>\n",
       "      <td>330</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K4_jpg.rf.00821732715c9137b8060360770ea1d8.jpg</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>K</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>351</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>W6_jpg.rf.00d19bc3a49f6469e2afa3aa92f14ff4.jpg</td>\n",
       "      <td>412</td>\n",
       "      <td>412</td>\n",
       "      <td>W</td>\n",
       "      <td>22</td>\n",
       "      <td>73</td>\n",
       "      <td>377</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J30_jpg.rf.00d20e595026b31773ded47509545471.jpg</td>\n",
       "      <td>382</td>\n",
       "      <td>382</td>\n",
       "      <td>J</td>\n",
       "      <td>122</td>\n",
       "      <td>204</td>\n",
       "      <td>250</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename  width  height class  xmin  \\\n",
       "0   U7_jpg.rf.0037faea78f8a89329a93006132921b3.jpg    390     390     U    38   \n",
       "1  P12_jpg.rf.0046c1c30abbbccd31716c5b2ad835b9.jpg    372     372     P    84   \n",
       "2   K4_jpg.rf.00821732715c9137b8060360770ea1d8.jpg    372     372     K    42   \n",
       "3   W6_jpg.rf.00d19bc3a49f6469e2afa3aa92f14ff4.jpg    412     412     W    22   \n",
       "4  J30_jpg.rf.00d20e595026b31773ded47509545471.jpg    382     382     J   122   \n",
       "\n",
       "   ymin  xmax  ymax  \n",
       "0    69   347   389  \n",
       "1   203   330   332  \n",
       "2    12   351   369  \n",
       "3    73   377   412  \n",
       "4   204   250   334  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: MAke csv as described\n",
    "train = pd.read_csv(\"./data/American Sign Language Letters.v1-v1.tensorflow/train/_annotations.csv\")\n",
    "test = pd.read_csv(\"./data/American Sign Language Letters.v1-v1.tensorflow/test/_annotations.csv\")\n",
    "valid = pd.read_csv(\"./data/American Sign Language Letters.v1-v1.tensorflow/valid/_annotations.csv\")\n",
    "train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_16056\\2743454335.py:59: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_feat = np.array(test_feat)\n"
     ]
    }
   ],
   "source": [
    "def getFeatures(files,root,labels):\n",
    "    \"\"\" \n",
    "    Reads in a set of image files and returns a list of the hand_features for each\n",
    "    \n",
    "    \"\"\"\n",
    "    features = [] # store features here\n",
    "    labels_true = [] # We make sure that the corresponding label is there. Covers the case where no_hand is detected\n",
    "\n",
    "    IMAGE_FILES = files\n",
    "    with mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.5) as hands:\n",
    "        for idx, file_ori in enumerate(IMAGE_FILES):\n",
    "            file = root + file_ori # ex root: \"./data/unlabeled/\" \n",
    "            # print(\"iteration\",idx,\"file,\",file)\n",
    "            # Read an image, flip it around y-axis for correct handedness output (see\n",
    "            # above).\n",
    "\n",
    "            # image = cv2.flip(cv2.imread(file), 1)\n",
    "            image = cv2.imread(file)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Convert the BGR image to RGB before processing.\n",
    "            results = hands.process(image)\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            # Extract the hand landmarks\n",
    "            landmarks = []\n",
    "            if results.multi_hand_landmarks:\n",
    "                #Label update:\n",
    "                labels_true.append(labels[idx])\n",
    "                #Hand features updates\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Get the bounding box of the hand\n",
    "                    x_min, y_min, x_max, y_max = float('inf'), float('inf'), float('-inf'), float('-inf')\n",
    "                    for landmark in hand_landmarks.landmark:\n",
    "                        x, y = landmark.x, landmark.y\n",
    "                        x_min = min(x_min, x)\n",
    "                        y_min = min(y_min, y)\n",
    "                        x_max = max(x_max, x)\n",
    "                        y_max = max(y_max, y)\n",
    "                    \n",
    "                    # Normalize the coordinates with respect to the bounding box of the hand\n",
    "                    for landmark in hand_landmarks.landmark:\n",
    "                        x, y, z = landmark.x, landmark.y, landmark.z\n",
    "                        x_norm = (x - x_min) / (x_max - x_min)\n",
    "                        y_norm = (y - y_min) / (y_max - y_min)\n",
    "                        landmarks.append([x_norm, y_norm, z])\n",
    "\n",
    "            # Convert the landmarks to a feature vector\n",
    "            feature_vector = np.array(landmarks)\n",
    "            features.append(feature_vector)\n",
    "            # Print the feature vector\n",
    "            # print(feature_vector)\n",
    "    return features, labels_true\n",
    "            \n",
    "test_feat, labels = getFeatures(['hands.jpg','hands2.jpg'],\"./data/unlabeled/\",[\"A\",\"B\"])\n",
    "test_feat = np.array(test_feat).reshape(len(test_feat,))\n",
    "print(test_feat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sends list of file names, the source directory, and the labels list\n",
    "x_train, y_train = getFeatures(train.iloc[:,0],\"./data/American Sign Language Letters.v1-v1.tensorflow/train/\", train.iloc[:,3].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(data=x_train)\n",
    "df_train.to_csv(\"./data/American Sign Language Letters.v1-v1.tensorflow/train/train_features.csv\")\n",
    "\n",
    "df_train = pd.DataFrame(data=y_train)\n",
    "df_train.to_csv(\"./data/American Sign Language Letters.v1-v1.tensorflow/train/train_labels.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(letters):\n",
    "    \"\"\" \n",
    "    One hot encodes letter labels\n",
    "    \"\"\"\n",
    "    hot_labels = []\n",
    "    for letter in letters:\n",
    "        encoded = [0]*26\n",
    "        #Check if invalid character\n",
    "        if (ord(letter) < 65) or (ord(letter) > 122):\n",
    "            encoded = [-1]*26\n",
    "            print(\"Warning: Invalid character detected\")\n",
    "        #Otherwise valid and update encoded vec    \n",
    "        else:\n",
    "            if ord(letter) <96: #Upper case\n",
    "                encoded[ord(letter)-65] = 1\n",
    "            else: #Lowercase\n",
    "                encoded[ord(letter)-97] = 1\n",
    "        hot_labels.append(encoded)\n",
    "    return hot_labels\n",
    "## TEST\n",
    "letters_unit_test = [\"A\", \"a\",\"Z\",\"z\",\"L\"]\n",
    "oneHot(letters_unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode\n",
    "y_train_hot = oneHot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

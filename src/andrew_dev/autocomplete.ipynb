{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocompletion Using Fast autocomplete\n",
    "Fast autocomplete uses a DWG to search through words that can be fed into an autocomplete object type. As such, we must both populate and rank words to feed into our object before we can use autocomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, words\n",
    "from fast_autocomplete import AutoComplete\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the library\n",
    "Run this if the word_dict.json file has not been made yet. Otherwise, skip to the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, words\n",
    "from fast_autocomplete import AutoComplete\n",
    "import string\n",
    "\n",
    "# Load the Brown Corpus\n",
    "nltk.download('brown')\n",
    "corpus = brown.words()\n",
    "\n",
    "\n",
    "# Want to filter to only valid words. We use valid words as this is a default\n",
    "with open('valid_words.txt', 'r') as file:\n",
    "    valid_words_raw = file.read().splitlines()\n",
    "valid_words = [word.lower() for word in valid_words_raw]\n",
    "\n",
    "\n",
    "\n",
    "# # Count the frequency of each word\n",
    "word_freq = nltk.FreqDist(corpus)\n",
    "\n",
    "# # Create a list of words sorted by frequency\n",
    "# Note we remove all cases where words have punctuation besides ' and - \n",
    "corpus_raw = [word.lower() for word in corpus if all(c not in string.punctuation or c in [\"'\", \"-\"] for c in word)]\n",
    "corpus = corpus_raw # Can filter here if desired\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48484"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get frequency distribution\n",
    "word_freq = nltk.FreqDist(corpus)\n",
    "len(word_freq.most_common()) # number of unique words from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [{}, 'the', 69971]\n",
      "of [{}, 'of', 36412]\n",
      "and [{}, 'and', 28853]\n",
      "to [{}, 'to', 26158]\n",
      "in [{}, 'in', 21337]\n",
      "that [{}, 'that', 10594]\n",
      "is [{}, 'is', 10109]\n",
      "was [{}, 'was', 9815]\n",
      "he [{}, 'he', 9548]\n",
      "for [{}, 'for', 9489]\n",
      "number of words: 22378\n"
     ]
    }
   ],
   "source": [
    "# Create a list of words sorted by frequency\n",
    "words = [(word, freq) for word, freq in word_freq.most_common(30000) if word in valid_words]\n",
    "# Create a dictionary of words in the required format\n",
    "word_dict = {}\n",
    "words_new = {}\n",
    "for word, freq in words:\n",
    "    word_dict[word] = [{}, word, freq]\n",
    "    words_new[word] = {'count':freq}\n",
    "\n",
    "# Print the first 10 words in the dictionary\n",
    "for word, data in list(word_dict.items())[:10]:\n",
    "    print(word, data)\n",
    "print(\"number of words:\",len(word_dict.items()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first try storing the words into a json and load directly from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY RUN IF WE WANT TO REPLACE THE CURRENT JSON DIctionary!!!\n",
    "# import json\n",
    "# # Convert the word_dict to a JSON string\n",
    "# word_dict_json = json.dumps(word_dict)\n",
    "\n",
    "# # Write the JSON string to a file\n",
    "# with open('word_dict.json', 'w') as f:\n",
    "#     f.write(word_dict_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the autocomplete\n",
    "Now we can create an autocomplete object using the json file and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_autocomplete import autocomplete_factory\n",
    "\n",
    "content_files = {\n",
    "    'words': {\n",
    "        'filepath': 'word_dict.json',\n",
    "        'compress': True  # means compress the graph data in memory\n",
    "    }\n",
    "}\n",
    "\n",
    "autocomplete = autocomplete_factory(content_files=content_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['crash'], ['crashed'], ['crashing'], ['crass'], ['crashes']]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autocomplete.search(word='cras',size=10, max_cost=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is to troubleshoot any missing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potato is valid and in corpus\n"
     ]
    }
   ],
   "source": [
    "# If a word is missing, we can check if it is vaid words, or just did not occur in the corpus\n",
    "test_word = \"potato\"\n",
    "if test_word in valid_words:\n",
    "    try:\n",
    "        autocomplete.words[test_word]\n",
    "        print(test_word, \"is valid and in corpus\")\n",
    "    except:\n",
    "        print(test_word,\"is valid but NOT in the corpus\")\n",
    "else:\n",
    "    print(test_word,\"not in valid word set\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw DWG\n",
    "We can use this to generate a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You need to install pygraphviz in order to draw graphs\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'pgv' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m      7\u001b[0m autocomplete \u001b[39m=\u001b[39m AutoCompleteDraw(words\u001b[39m=\u001b[39mwords_new)\n\u001b[1;32m----> 8\u001b[0m autocomplete\u001b[39m.\u001b[39;49mdraw_graph(\u001b[39m'\u001b[39;49m\u001b[39mgraph.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fast_autocomplete\\draw.py:30\u001b[0m, in \u001b[0;36mDrawGraphMixin.draw_graph\u001b[1;34m(self, file_path, starting_word, agraph_kwargs, prog)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mYou need to install pygraphviz in order to draw graphs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m agraph_kwargs \u001b[39m=\u001b[39m agraph_kwargs \u001b[39mif\u001b[39;00m agraph_kwargs \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m---> 30\u001b[0m graph \u001b[39m=\u001b[39m pgv\u001b[39m.\u001b[39mAGraph(strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, directed\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39magraph_kwargs)\n\u001b[0;32m     32\u001b[0m edges \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m     33\u001b[0m que \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mdeque()\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'pgv' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from fast_autocomplete import AutoComplete, DrawGraphMixin\n",
    "\n",
    "\n",
    "class AutoCompleteDraw(DrawGraphMixin, AutoComplete):\n",
    "    pass\n",
    "\n",
    "autocomplete = AutoCompleteDraw(words=words_new)\n",
    "autocomplete.draw_graph('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
